## 今日收获  
- 1.相关基本概念的完善：  
（1）L2正则化：即在损失函数表达式中为需要约束的变量添加一λ∑w2项，λ即为约束变量，λ越大，可以约束w越小。是一种常见的防止过拟合的手段。  
（2）归一化：即将向量大小约束为1，确保不同样本具有相同的尺度。  
（3）余弦相似度：度量两个向量方向相似程度的指标，就是高中所学的利用向量点积计算夹角的cosθ。主要用于计算相关事物（如语句、图片）的相似度。  
（4）Pytorch中的张量：基本可以等于数组，Tensor[3][3]即代表着3×3的一个数组，只不过可以让GPU参与计算等等，相当于“深度学习领域的数组”。

- 2.SupCon损失：
其核心在于正样本对与负样本对之间的特征计算，通过选定一列锚点（在SimGCD中即每个样本的视图，可能为全部视图，也可能为部分视图），再通过比较锚点与所有样本的相似度，提高正样本相似度、减小负样本相似度，使得对比学习更加有效。其既可以用于有监督学习（利用Label，直接比较相同类别、不同类别，省去了对mask操作的步骤），也可以用于无监督学习（利用mask，一个n×n的矩阵，来映射样本对之间的关系，进而区分正负样本对）。
- 3.infoNCE损失：
与SupCon损失原理上几乎一样，都是通过构造损失函数，希望能够让正样本对离得更近，负样本对离得更远来使得对比学习更加有效。但infoNCE不同的是，它需要直接确认正负样本，而不是通过相似度来判断哪些才是正负样本，比如在SimGCD中，由一个样本经过数据增强得到的两个视图，那它们两个就是正样本，其余的视图相对于它们来说都是负样本。而后通过计算相似度，使得正样本更近、负样本更远。
